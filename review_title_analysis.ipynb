{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "review_title_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HmXE90aFoqPe"
      ],
      "toc_visible": true,
      "mount_file_id": "1y13Ipc_OZ0Bw9EC9K55hSTmqkS3-_fTO",
      "authorship_tag": "ABX9TyOdQKjGzk4e8WfaH52ubFF5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgJ7JOd1VCik"
      },
      "source": [
        "## Analysis on title\n",
        "In my previous work, I calculated an overall score based on Treatment Rating, Number of photos, Number of words and Provider rating, without involving column of Title. I figured this data is probably useful, at least couldn't be ignored, so I try to apply latent semantic analysis (LSA) to these customers' review titles. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci0ty6VnwuOR"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # convert words into weight matrix \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf_5l14wwy3M"
      },
      "source": [
        "# Import data posted in question 2\n",
        "df = pd.read_excel('/content/drive/MyDrive/Q2.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-enFA2cjPvB",
        "outputId": "061968a5-1f78-4885-e942-d4491a548e64"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 662 entries, 0 to 661\n",
            "Data columns (total 9 columns):\n",
            " #   Column            Non-Null Count  Dtype         \n",
            "---  ------            --------------  -----         \n",
            " 0   Created Date      662 non-null    datetime64[ns]\n",
            " 1   Treatment Rating  662 non-null    object        \n",
            " 2   Number of Photos  662 non-null    int64         \n",
            " 3   Number of Words   662 non-null    int64         \n",
            " 4   Provider Rating   644 non-null    float64       \n",
            " 5   Physician Type    547 non-null    object        \n",
            " 6   Treatment Name    662 non-null    object        \n",
            " 7   Title             662 non-null    object        \n",
            " 8   Cost              662 non-null    float64       \n",
            "dtypes: datetime64[ns](1), float64(2), int64(2), object(4)\n",
            "memory usage: 46.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "57FxyPUvUauw",
        "outputId": "999b1233-14b7-453b-9d00-276457d8ceff"
      },
      "source": [
        "# Preview of data\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Created Date</th>\n",
              "      <th>Treatment Rating</th>\n",
              "      <th>Number of Photos</th>\n",
              "      <th>Number of Words</th>\n",
              "      <th>Provider Rating</th>\n",
              "      <th>Physician Type</th>\n",
              "      <th>Treatment Name</th>\n",
              "      <th>Title</th>\n",
              "      <th>Cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-05-31</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>0</td>\n",
              "      <td>109</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Physician</td>\n",
              "      <td>volbella</td>\n",
              "      <td>From Nervous to Loyal Customer in One Visit!</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-02-13</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>0</td>\n",
              "      <td>96</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Dermatologic Surgeon</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Dr. Schlessinger did a fantastic job giving me...</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-01-13</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>2</td>\n",
              "      <td>93</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Plastic Surgeon</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Amazing Artistry!</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-06-30</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Family Physician</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Expert Injector!</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-03-21</td>\n",
              "      <td>Not worth it</td>\n",
              "      <td>3</td>\n",
              "      <td>172</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Late Reaction to Volbella</td>\n",
              "      <td>750.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2019-01-03</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Facial Plastic Surgeon</td>\n",
              "      <td>volbella</td>\n",
              "      <td>I had my lips done, simply amazing!</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2019-02-14</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Plastic Surgeon</td>\n",
              "      <td>volbella</td>\n",
              "      <td>I Had a Wonderful Experience</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2019-04-06</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>3</td>\n",
              "      <td>94</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Wanted a natural, but fuller look to my lips</td>\n",
              "      <td>449.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2019-05-26</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Oculoplastic Surgeon</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Dark Circle Fillers</td>\n",
              "      <td>2200.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019-03-19</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>0</td>\n",
              "      <td>115</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Plastic Surgeon</td>\n",
              "      <td>volbella</td>\n",
              "      <td>5 Star Experience</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Created Date  ...     Cost\n",
              "0   2019-05-31  ...     0.00\n",
              "1   2019-02-13  ...     0.00\n",
              "2   2019-01-13  ...     0.00\n",
              "3   2019-06-30  ...     0.00\n",
              "4   2019-03-21  ...   750.00\n",
              "5   2019-01-03  ...     0.00\n",
              "6   2019-02-14  ...     0.00\n",
              "7   2019-04-06  ...   449.98\n",
              "8   2019-05-26  ...  2200.00\n",
              "9   2019-03-19  ...     0.00\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmXE90aFoqPe"
      },
      "source": [
        "#### Latent semantic analysis on titles \n",
        "\n",
        "Suppose that some consumers use words which could express strong emotions, either positive or negative, like fantasitcs, amazing, or horrible, disappointed. On the contrary, others may just choose neutral words, such as natural. So I will try to cluster the review titles, label them and give each title a score denpending on how much consumers like the treatment or hate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M21wVzkcSEm1"
      },
      "source": [
        "# Define all 662 titles as corpus and make sure all words in lowercase.\n",
        "corpus = df.Title.str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_iqokyjIJRZ"
      },
      "source": [
        "# Convert all words in review title, apart from stop words in English, into term frequency–inverse document frequency (TF-IDF) matrix.\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english' ) \n",
        "vectors = vectorizer.fit_transform(corpus) \n",
        "words = vectorizer.get_feature_names()                                             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxR-QPX8H1ME",
        "outputId": "7b331375-88a3-48ef-c23f-cb26dc328e4c"
      },
      "source": [
        "print('There are %s distinct words(features) in review titles.' %len(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 727 distinct words(features) in review titles.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o5FGgLjzici",
        "outputId": "4b246e0a-3eca-4857-b36b-90f48937327c"
      },
      "source": [
        "print(words) # All 727 words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['10', '100', '11', '11s', '135', '15', '1st', '22', '28', '30', '3rd', '40', '59', 'aaa', 'absolutely', 'accents', 'accommodating', 'accurate', 'acid', 'actually', 'adams', 'addicted', 'adorable', 'advice', 'advocate', 'aesthetics', 'affordable', 'african', 'agility', 'aging', 'ago', 'ahead', 'aka', 'alecia', 'allergic', 'allison', 'altered', 'amanda', 'amazing', 'american', 'amp', 'angelos', 'angry', 'answer', 'anwar', 'anymore', 'appearance', 'appointment', 'area', 'areas', 'art', 'artist', 'artistry', 'ask', 'asked', 'aspect', 'assist', 'atkins', 'atmosphere', 'attention', 'augmentation', 'authentic', 'awesome', 'bad', 'based', 'beautiful', 'beauty', 'bech', 'bedside', 'believe', 'believer', 'bello', 'berman', 'best', 'bestie', 'better', 'beverly', 'bigger', 'biz', 'blame', 'bloomfield', 'bodylase', 'boll', 'bone', 'booster', 'botox', 'botoxer', 'bourre', 'bourree', 'boutique', 'breast', 'bright', 'brilliant', 'bringing', 'brings', 'brookes', 'brotox', 'brought', 'brow', 'bruise', 'bruising', 'business', 'busy', 'bye', 'calm', 'campano', 'caps', 'capuano', 'care', 'careful', 'cares', 'caring', 'carrody', 'cases', 'catherine', 'caused', 'cautious', 'center', 'centre', 'changing', 'charlotte', 'chemical', 'cheryl', 'chesnut', 'chestnut', 'chia', 'chin', 'chiu', 'cho', 'choe', 'choice', 'chou', 'christine', 'circle', 'city', 'class', 'clean', 'clenching', 'client', 'clinic', 'closer', 'coe', 'come', 'comes', 'comfortable', 'comforting', 'coming', 'compassion', 'competent', 'completely', 'confidence', 'considerate', 'considering', 'consistent', 'consistently', 'consult', 'consultation', 'contenta', 'convenient', 'cool', 'coolmini', 'coolsculpting', 'coolscupting', 'correction', 'cosmetic', 'couldn', 'county', 'courteous', 'craft', 'creases', 'croton', 'crows', 'customer', 'daily', 'dana', 'dark', 'day', 'debbi', 'decided', 'decor', 'deep', 'definitely', 'dent', 'dermaroller', 'dermatologist', 'dermatology', 'detox', 'did', 'didn', 'diego', 'difference', 'disappointed', 'disappointing', 'disgusting', 'doc', 'docs', 'doctor', 'doctors', 'does', 'doing', 'don', 'dont', 'double', 'dr', 'dreams', 'drive', 'dull', 'eager', 'earlier', 'earned', 'ease', 'easy', 'efficient', 'ende', 'entire', 'environment', 'enzyme', 'estee', 'esthetician', 'estheticians', 'eve', 'everytime', 'exceeded', 'excellence', 'excellent', 'exceptional', 'exceptionally', 'excited', 'expectation', 'expectations', 'expected', 'experience', 'experienced', 'experiences', 'experimented', 'expert', 'expertise', 'expired', 'expressions', 'extraordinaire', 'extremely', 'eye', 'eyes', 'fabulous', 'face', 'facial', 'facials', 'facility', 'falcone', 'family', 'fantastic', 'far', 'fast', 'fat', 'fav', 'feel', 'feet', 'felt', 'female', 'field', 'filler', 'fillers', 'finally', 'fine', 'flip', 'forehead', 'forever', 'fort', 'fountain', 'free', 'fresh', 'friendly', 'friends', 'frown', 'frustration', 'fuller', 'fun', 'gardner', 'gave', 'gem', 'gentle', 'genuine', 'getaway', 'gets', 'getting', 'gift', 'girl', 'gives', 'giving', 'glabella', 'glow', 'glowing', 'god', 'goes', 'going', 'gone', 'good', 'gorgeous', 'got', 'gotten', 'gr8', 'gracefully', 'grandma', 'granny', 'grateful', 'great', 'growing', 'hand', 'hands', 'happened', 'happy', 'hassle', 'hastings', 'haus', 'hause', 'having', 'hayre', 'heather', 'heck', 'helen', 'hello', 'help', 'helpful', 'hetter', 'highly', 'hills', 'ho', 'home', 'honest', 'horrible', 'huddersfield', 'imagination', 'imagined', 'impressed', 'impressive', 'improved', 'incredible', 'informative', 'injectables', 'injecter', 'injection', 'injections', 'injector', 'inner', 'integrity', 'isn', 'jacqueline', 'jaw', 'jeanna', 'jeannine', 'jennifer', 'job', 'julia', 'june', 'just', 'juvaderm', 'juvly', 'karcher', 'kass', 'kate', 'keeps', 'kept', 'kevin', 'key', 'kind', 'king', 'kirsten', 'knowledgable', 'knowledge', 'knowledgeable', 'knows', 'la', 'lactic', 'ladies', 'laser', 'lasting', 'lasts', 'late', 'later', 'lauren', 'lbs', 'lean', 'learn', 'left', 'let', 'level', 'life', 'like', 'line', 'lines', 'lip', 'lips', 'lisa', 'lives', 'll', 'location', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lori', 'love', 'lovely', 'loyal', 'loyalty', 'lt', 'lucy', 'mabrie', 'maine', 'make', 'makes', 'manner', 'marotta', 'masseter', 'master', 'matthew', 'mbla', 'med', 'medi', 'medical', 'medium', 'meh', 'mendes', 'mermaizing', 'microblading', 'miller', 'mimi', 'mini', 'mirza', 'miscommunication', 'mizgala', 'modern', 'moenster', 'mom', 'morgan', 'moscatello', 'motto', 'muscle', 'naderi', 'nahabedian', 'natural', 'neah', 'neck', 'need', 'needed', 'needs', 'nervous', 'new', 'newburyport', 'nice', 'nicest', 'nicole', 'nightmare', 'nima', 'nose', 'notch', 'nowak', 'number', 'nurse', 'nursing', 'nyc', 'obvious', 'office', 'oil', 'ok', 'old', 'olga', 'oscar', 'outcome', 'outer', 'outstanding', 'pa', 'painless', 'parking', 'parsa', 'patel', 'patient', 'patients', 'pca', 'peel', 'people', 'perfect', 'performs', 'person', 'personable', 'personal', 'physician', 'place', 'plan', 'plastic', 'plastics', 'pleasant', 'pleasantly', 'pleased', 'pleasing', 'plus', 'polaris', 'positive', 'post', 'pousti', 'practitioner', 'praise', 'pre', 'precise', 'prices', 'procedure', 'procedures', 'process', 'product', 'products', 'professional', 'prominent', 'prompt', 'provider', 'provides', 'puffy', 'queen', 'quick', 'rachelle', 'raval', 'reaction', 'reactions', 'reactivator', 'real', 'really', 'receiving', 'recomend', 'recommend', 'recommended', 'refreshed', 'refresher', 'refreshing', 'regular', 'regularly', 'rejuva', 'rejuvenated', 'rejuvenation', 'relaxed', 'relaxing', 'removal', 'renewed', 'resolved', 'respect', 'restarting', 'rested', 'result', 'results', 'returning', 'review', 'rewarding', 'ricciardelli', 'richardson', 'rivkin', 'rocks', 'rogal', 'routinely', 'salazar', 'salon', 'san', 'sandhu', 'sara', 'sarah', 'sari', 'satisfied', 'sattler', 'say', 'saying', 'schlessinger', 'scrunched', 'search', 'second', 'seen', 'seriously', 'service', 'services', 'severe', 'shah', 'shaurya', 'simple', 'simply', 'single', 'skill', 'skilled', 'skills', 'skin', 'skincare', 'slimming', 'soft', 'sohrab', 'sonoma', 'sound', 'spa', 'specialty', 'spectacular', 'stacy', 'staff', 'standards', 'star', 'started', 'state', 'stef', 'stefanie', 'steitz', 'stephanie', 'stern', 'stop', 'stops', 'success', 'sullivan', 'super', 'surgeon', 'surgery', 'surprised', 'surprisingly', 'suture', 'sweet', 'sylvia', 'sylvie', 'sympathetic', 'taban', 'talent', 'talented', 'tampa', 'tattelbaum', 'taylor', 'tchelebi', 'team', 'tell', 'terrible', 'terrific', 'texas', 'thank', 'thanks', 'thee', 'things', 'thorough', 'thoughtful', 'thrilled', 'time', 'timer', 'times', 'tired', 'told', 'torgerson', 'total', 'touch', 'treament', 'treated', 'treatment', 'treatments', 'treats', 'tremendous', 'trip', 'true', 'truly', 'trust', 'trusted', 'try', 'tsay', 'tune', 'ultimate', 'understanding', 'upper', 'upscale', 'utah', 'vartanys', 've', 'vega', 'vertigo', 'vip', 'visit', 'visited', 'volbella', 'vollure', 'wait', 'waiting', 'want', 'wanted', 'warm', 'way', 'ways', 'week', 'weiler', 'welcoming', 'wendy', 'went', 'west', 'williams', 'winter', 'wish', 'wizard', 'won', 'wonderful', 'wonderfully', 'wonders', 'wont', 'woodyard', 'work', 'works', 'world', 'worth', 'wouldn', 'wouldnt', 'wow', 'wrinkles', 'wrong', 'xl', 'yarinsky', 'yay', 'year', 'years', 'yes', 'yesterday', 'york', 'younger', 'youth', 'youthful', 'äã', 'äî', 'äîvery', 'äôll', 'äôm', 'äôs', 'äôt', 'äôve']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct_oje5Jz7jg",
        "outputId": "f8653966-707e-497e-db6b-b79f3917e999"
      },
      "source": [
        "# print 1st vector in the matrix. \n",
        "# Only 4 non-zero elements, corresponding 4 words in first title(apart from stop words), nervous, loyal, customer, visit.\n",
        "# Most of elements in matrix are 0, so it's a sparse matrix. Need to be downsized.\n",
        "print(vectors.toarray()[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.44051504 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.55324273 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.520272   0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.47873378 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQcOuIRh4ANb",
        "outputId": "a9b87366-7707-441a-a341-37cf222e157f"
      },
      "source": [
        "# Use truncated SVD to downsize the matrix\n",
        "\n",
        "categories = 5        # Remain 5 columns, which means review titles will be classified into 5 categories by similarity. \n",
        "lsa = TruncatedSVD(n_components=categories)  \n",
        "trunc_v = lsa.fit_transform(vectors)  \n",
        "print(\"--------lsa singular value---------\")\n",
        "print(lsa.singular_values_)\n",
        "print(\"--------662 review titles，in %s categories vector space---------\" %categories)\n",
        "print(trunc_v.shape)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------lsa singular value---------\n",
            "[5.85804978 4.76231525 4.32992423 3.68126854 3.54158413]\n",
            "--------662 review titles，in 5 categories vector space---------\n",
            "(662, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttk_r2Gr4nbT"
      },
      "source": [
        "# Pick 5 most typical titles in each category.\n",
        "\n",
        "pick_titles = 5  \n",
        "title_docid = [trunc_v[:, i].argsort()[:-(pick_titles + 1):-1] for i in range(categories)]\n",
        "#print(\"--------5 most typical titles in each category---------\")\n",
        "#print(title_docid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ee2VRdw7Mq"
      },
      "source": [
        "# Pick 5 key words in each category\n",
        "\n",
        "pick_keywords = 5  \n",
        "cat_keywdid = [lsa.components_[i].argsort()[:-(pick_keywords + 1):-1] for i in range(categories)]\n",
        "#print(\"--------3 typical words in each category---------\")\n",
        "#print(cat_keywdid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lDQXlqSwCB1",
        "outputId": "4fc39aeb-a11c-4b76-e51a-1d4fcaa65ba4"
      },
      "source": [
        "print(\"-------- Results---------\")\n",
        "for c in range(categories):\n",
        "    print(\"\\n rating categories {}\".format(c+1))\n",
        "    print(\"\\t keywords：{}\".format(\", \".join(words[cat_keywdid[c][j]] for j in range(pick_keywords))))\n",
        "    for i in range(pick_titles):\n",
        "        print('\\t\\t titles %s: %s,' % ('{}'.format(i+1),corpus[title_docid[c][i]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------- Results---------\n",
            "\n",
            " rating categories 1\n",
            "\t keywords：experience, amazing, great, botox, results\n",
            "\t\t titles 1: amazing experience,\n",
            "\t\t titles 2: amazing experience...,\n",
            "\t\t titles 3: i had an amazing experience,\n",
            "\t\t titles 4: amazing experience,\n",
            "\t\t titles 5: amazing experience,\n",
            "\n",
            " rating categories 2\n",
            "\t keywords：botox, best, great, results, treatment\n",
            "\t\t titles 1: botox,\n",
            "\t\t titles 2: botox,\n",
            "\t\t titles 3: botox,\n",
            "\t\t titles 4: botox,\n",
            "\t\t titles 5: botox,\n",
            "\n",
            " rating categories 3\n",
            "\t keywords：great, experience, service, results, wonderful\n",
            "\t\t titles 1: great experience, great results!,\n",
            "\t\t titles 2: another great experience,\n",
            "\t\t titles 3: always a great experience!!,\n",
            "\t\t titles 4: great experience,\n",
            "\t\t titles 5: great experience!,\n",
            "\n",
            " rating categories 4\n",
            "\t keywords：best, experience, doctor, dr, plastic\n",
            "\t\t titles 1: the best,\n",
            "\t\t titles 2: best of the best,\n",
            "\t\t titles 3: best of the best!,\n",
            "\t\t titles 4: best experience!,\n",
            "\t\t titles 5: best botox! best doctor! best medi spa!,\n",
            "\n",
            " rating categories 5\n",
            "\t keywords：results, excellent, love, staff, happy\n",
            "\t\t titles 1: excellent results,\n",
            "\t\t titles 2: excellent results,\n",
            "\t\t titles 3: excellent results,\n",
            "\t\t titles 4: love the results!,\n",
            "\t\t titles 5: love the results,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attIhu47HCIc"
      },
      "source": [
        "As the result shows, titles are classified into 5 categories, based on how strongly they related to each category, or each particular topic. However, I'm facing several problems. The first problem is, although there are 5 categories, I can't state that one category represents stronger emotion than another does, in other words, I'm unable to grade the categories. \n",
        "\n",
        "The 2nd problem is that there're too many identical key words in different categories, so the classification is not performed well. This is easy to be understood because most customers selected same words to express their happiness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoPRH8DjmVXx"
      },
      "source": [
        "#### I noticed that there are no negative words in any category, Thus, this time, I'll analyze the titles written by consumers giving 'Not worth it' only. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwhWt0wCGVEo",
        "outputId": "b6a15f60-e64a-4ee2-bb18-8592dc5a962e"
      },
      "source": [
        "# Extract titles written by users give negative rating.\n",
        "df_neg = df[df['Treatment Rating'] == 'Not worth it']\n",
        "df_neg.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 31 entries, 4 to 633\n",
            "Data columns (total 9 columns):\n",
            " #   Column            Non-Null Count  Dtype         \n",
            "---  ------            --------------  -----         \n",
            " 0   Created Date      31 non-null     datetime64[ns]\n",
            " 1   Treatment Rating  31 non-null     object        \n",
            " 2   Number of Photos  31 non-null     int64         \n",
            " 3   Number of Words   31 non-null     int64         \n",
            " 4   Provider Rating   23 non-null     float64       \n",
            " 5   Physician Type    20 non-null     object        \n",
            " 6   Treatment Name    31 non-null     object        \n",
            " 7   Title             31 non-null     object        \n",
            " 8   Cost              31 non-null     float64       \n",
            "dtypes: datetime64[ns](1), float64(2), int64(2), object(4)\n",
            "memory usage: 2.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2lQHPCnxUAG"
      },
      "source": [
        "# Generating a matrix of word occurences instead of TF-IDF value.\n",
        "\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "neg_v = cv.fit_transform(df_neg.Title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W97rLzgXU7E",
        "outputId": "58624aba-cc45-4d58-9993-dd7c06055fd5"
      },
      "source": [
        "word_list = cv.get_feature_names()\n",
        "count_list = neg_v.toarray().sum(axis=0) \n",
        "print('%s words (features) in negative review titles' %len(word_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77 words (features) in negative review titles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkScPslaXwM5",
        "outputId": "f7796d07-fee1-44e8-ed29-c1621663a8e3"
      },
      "source": [
        "#print top 10 words with highest occurence.\n",
        "\n",
        "d = dict(zip(word_list,count_list))\n",
        "print(sorted(d.items(), key=lambda item: item[1],reverse=True)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('botox', 12), ('experience', 5), ('horrible', 4), ('bad', 3), ('results', 3), ('crows', 2), ('feet', 2), ('jaw', 2), ('just', 2), ('masseter', 2)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ3_cSiLbyoT"
      },
      "source": [
        "In the negative review titles, the words about consumers sentiment, like horrible and bad, just appears 4 times and 3 times, respectively.\n",
        "So the last problem is, since there are only 31 consumers giving negative ratings, the key words in their reviews are eliminated during truncated SVD step because of low frequency. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znv3qViwE3SU"
      },
      "source": [
        "### Sentiment Intensity Analyzer\n",
        "\n",
        "LSA doen't work well in this situation, so I try to assign polarity score to each comment title leveraging built-in sentiment analyzer in the NLTK Python library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVztRErv81UU"
      },
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgskQPol-HTn",
        "outputId": "8ccc0559-9318-441a-b5f7-5b97ca1188bb"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTaZjRsM-EsY"
      },
      "source": [
        "sia = SIA() #Instantiate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5EEq2q-HmcF"
      },
      "source": [
        "ss_df = pd.DataFrame(columns=['neg','pos','neu','compound'],dtype='float')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "UnB2L4JVICxI",
        "outputId": "a5457951-219d-46fd-bd54-be8fb38254d4"
      },
      "source": [
        "ss_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [neg, pos, neu, compound]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUUtx-XE9pa9"
      },
      "source": [
        "ss_df['neg']=0.0\n",
        "ss_df['pos']=0.0\n",
        "ss_df['neu']=0.0\n",
        "ss_df['compound']=0.0\n",
        "for index,row in df.iterrows():\n",
        "    ss = sia.polarity_scores(row['Title'])\n",
        "    df.at[index,'neg'] = ss['neg']\n",
        "    df.at[index,'pos'] = ss['pos']\n",
        "    df.at[index,'neu']= ss['neu']\n",
        "    df.at[index,'compound'] = ss['compound']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "lcH2YwyV_fbi",
        "outputId": "0883f746-c4b9-47ec-cea5-2254a79dc472"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Created Date</th>\n",
              "      <th>Treatment Rating</th>\n",
              "      <th>Number of Photos</th>\n",
              "      <th>Number of Words</th>\n",
              "      <th>Provider Rating</th>\n",
              "      <th>Physician Type</th>\n",
              "      <th>Treatment Name</th>\n",
              "      <th>Title</th>\n",
              "      <th>Cost</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-05-31</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>0</td>\n",
              "      <td>109</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Physician</td>\n",
              "      <td>volbella</td>\n",
              "      <td>From Nervous to Loyal Customer in One Visit!</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.522</td>\n",
              "      <td>0.3164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-02-13</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>0</td>\n",
              "      <td>96</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Dermatologic Surgeon</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Dr. Schlessinger did a fantastic job giving me...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.414</td>\n",
              "      <td>0.8176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-01-13</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>2</td>\n",
              "      <td>93</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Plastic Surgeon</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Amazing Artistry!</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.6239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-06-30</td>\n",
              "      <td>Worth it</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Family Physician</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Expert Injector!</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-03-21</td>\n",
              "      <td>Not worth it</td>\n",
              "      <td>3</td>\n",
              "      <td>172</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>volbella</td>\n",
              "      <td>Late Reaction to Volbella</td>\n",
              "      <td>750.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Created Date Treatment Rating  Number of Photos  ...    pos    neu compound\n",
              "0   2019-05-31         Worth it                 0  ...  0.295  0.522   0.3164\n",
              "1   2019-02-13         Worth it                 0  ...  0.586  0.414   0.8176\n",
              "2   2019-01-13         Worth it                 2  ...  0.804  0.196   0.6239\n",
              "3   2019-06-30         Worth it                 0  ...  0.000  1.000   0.0000\n",
              "4   2019-03-21     Not worth it                 3  ...  0.000  1.000   0.0000\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "ZzYB45h6C_Sq",
        "outputId": "8da101a5-0df4-45ac-e0c4-b67fc26dfd2b"
      },
      "source": [
        "df.groupby('Treatment Rating').describe()['compound']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Treatment Rating</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Not worth it</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.266603</td>\n",
              "      <td>0.313217</td>\n",
              "      <td>-0.5859</td>\n",
              "      <td>-0.5423</td>\n",
              "      <td>-0.4767</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.5719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Worth it</th>\n",
              "      <td>631.0</td>\n",
              "      <td>0.464708</td>\n",
              "      <td>0.308264</td>\n",
              "      <td>-0.5255</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.5859</td>\n",
              "      <td>0.6696</td>\n",
              "      <td>0.9380</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  count      mean       std  ...     50%     75%     max\n",
              "Treatment Rating                             ...                        \n",
              "Not worth it       31.0 -0.266603  0.313217  ... -0.4767  0.0000  0.5719\n",
              "Worth it          631.0  0.464708  0.308264  ...  0.5859  0.6696  0.9380\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q-LDgDcFoft"
      },
      "source": [
        "It seems like this polarity score matching the conusmers' rating.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VoD8KXBDOmF"
      },
      "source": [
        "# Save and download the sentiment score\n",
        "df[['neg','pos','neu','compound']].to_csv('ss_df.csv', encoding='utf-8',index=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yMzqhYruJtAH",
        "outputId": "3363065f-8e91-4863-db26-b715b462e407"
      },
      "source": [
        "files.download('ss_df.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_eb474cc0-4bb2-4ce5-b5ad-665b20372f0a\", \"ss_df.csv\", 14011)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}